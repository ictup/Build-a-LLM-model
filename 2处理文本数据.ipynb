{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ba36fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the-verdict.txt', <http.client.HTTPMessage at 0x1e8a147b250>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = (\"https://raw.githubusercontent.com/rasbt/\" \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\" \"the-verdict.txt\") \n",
    "file_path = \"the-verdict.txt\" \n",
    "urllib.request.urlretrieve(url, file_path)\n",
    "#含义：调用 urlretrieve() 函数，从指定的 url 下载内容，并保存为 file_path 指定的文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cb5f330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20479\n"
     ]
    }
   ],
   "source": [
    "with open('the-verdict.txt','r',encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(len(raw_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b332994f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1396b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13c2d48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e5113fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0c1a843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bcea2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cbec7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "# set 不包含重复项，可以用来去重\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "979d98fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "# enumerate的作用是：在遍历一个可迭代对象（如列表）时，\n",
    "# 既能取到元素本身，又能同时获得元素的索引（下标）。\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >=50 :\n",
    "      \tbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92f2ad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self,vocab):\n",
    "        # vocab是我们通过去重后的分词的一个字典\n",
    "        # 字典中的键值对都是token：索引\n",
    "        # 其中包含了一个数据集，按照单词划分的所有token\n",
    "        #A 将词汇表作为类属性存储，以方便在 encode 和 decode 方法中访问\n",
    "        #B 创建一个反向词汇表，将token ID 映射回原始的文本token\n",
    "\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s ,i in vocab.items()}\n",
    "        \n",
    "    def encode(self,text):\n",
    "        #C 将输入文本转换为token ID\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        # 只有当item.strip()不为空字符串的时候添加进列表\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids \n",
    "        # 返回索引\n",
    "    \n",
    "    def decode(self,ids):\n",
    "        #C 将输入文本转换为token ID\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) \n",
    "        #E 在指定的标点符号前去掉空格                          #E\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1943a9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 56,\n",
       " 2,\n",
       " 850,\n",
       " 988,\n",
       " 602,\n",
       " 533,\n",
       " 746,\n",
       " 5,\n",
       " 1126,\n",
       " 596,\n",
       " 5,\n",
       " 1,\n",
       " 67,\n",
       " 7,\n",
       " 38,\n",
       " 851,\n",
       " 1108,\n",
       " 754,\n",
       " 793,\n",
       " 7]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text =  \"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f216e93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " '\"It\\'s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.decode(ids)),text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecbc8e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n",
      "1132\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "# all_tokens.append(['<|endoftext|>','<|unk|>'])\n",
    "#append 是把一个元素整体加到列表末尾，这里你添加的是一个列表，结果就变成了嵌套列表：\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "for tok in ['<|unk|>', '<|endoftext|>']:\n",
    "    if tok not in all_tokens:\n",
    "        all_tokens.append(tok)\n",
    "print(len(vocab.items()))\n",
    "vocab = {token:index for index ,token in enumerate(all_tokens)}\n",
    "print(len(vocab.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ddf742",
   "metadata": {},
   "source": [
    "| 方法          | 功能           | 示例                  | 结果                    |\n",
    "| ----------- | ------------ | ------------------- | --------------------- |\n",
    "| `append(x)` | 添加一个元素（不展开）  | `lst.append([4,5])` | `lst = [1,2,3,[4,5]]` |\n",
    "| `extend(x)` | 添加多个元素（展开加入） | `lst.extend([4,5])` | `lst = [1,2,3,4,5]`   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dc9997f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|unk|>', 1130)\n",
      "('<|endoftext|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):  # ✅ 先切片，再 enumerate\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d44b690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_idx = vocab\n",
    "        self.idx_to_str = {index:token for token,index in vocab.items()}\n",
    "    \n",
    "    def encode(self,text):\n",
    "        preprocessed = re.split(r'(<\\|.*?\\|>|[,.?_!\"()\\']|--|\\s)', text)\n",
    "\n",
    "\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "        # 如果不是空字符 就添加进列表\n",
    "        preprocessed = [item if item in self.str_to_idx \n",
    "                        else '<|unk|>' for item in preprocessed]\n",
    "        # 用 <|unk|> 替换所有“词表外”的 token，\n",
    "        # 证后面能顺利用 self.str_to_idx[s] 做查找。\n",
    "        ids = [self.str_to_idx[s] for s in preprocessed]\n",
    "\n",
    "        return ids\n",
    "    \n",
    "    def decode(self,ids):\n",
    "        text = \" \".join([self.idx_to_str[i] for i in ids])\n",
    "\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)                    #B\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "313fd151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = ' <|endoftext|> '.join((text1, text2))\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41538544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1130, 5, 355, 1126, 628, 975, 10, 1131, 55, 988, 956, 984, 722, 988, 1130, 7]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "index = tokenizer.encode(text)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a28e8ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7d47f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1131\n"
     ]
    }
   ],
   "source": [
    "print('<|endoftext|>' in vocab)     # True 吗？\n",
    "print(vocab['<|endoftext|>'])       # 有合法 ID 吗？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58bd8226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', '\"']\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed[-2:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0dfa78",
   "metadata": {},
   "source": [
    "## 2.5 字节对编码（Byte pair encoding）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1bdae97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91e5aaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package tiktoken:\n",
      "\n",
      "NAME\n",
      "    tiktoken - # This is the public API of tiktoken\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _educational\n",
      "    _tiktoken\n",
      "    core\n",
      "    load\n",
      "    model\n",
      "    registry\n",
      "\n",
      "VERSION\n",
      "    0.9.0\n",
      "\n",
      "FILE\n",
      "    d:\\python\\lib\\site-packages\\tiktoken\\__init__.py\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " ['Encoding',\n",
       "  '__builtins__',\n",
       "  '__cached__',\n",
       "  '__doc__',\n",
       "  '__file__',\n",
       "  '__loader__',\n",
       "  '__name__',\n",
       "  '__package__',\n",
       "  '__path__',\n",
       "  '__spec__',\n",
       "  '__version__',\n",
       "  '_tiktoken',\n",
       "  'core',\n",
       "  'encoding_for_model',\n",
       "  'encoding_name_for_model',\n",
       "  'get_encoding',\n",
       "  'list_encoding_names',\n",
       "  'model',\n",
       "  'registry'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "help(tiktoken),dir(tiktoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a45cc296",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.encoding_for_model(\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06c98434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Encoding in module tiktoken.core object:\n",
      "\n",
      "class Encoding(builtins.object)\n",
      " |  Encoding(name: 'str', *, pat_str: 'str', mergeable_ranks: 'dict[bytes, int]', special_tokens: 'dict[str, int]', explicit_n_vocab: 'int | None' = None)\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getstate__(self) -> 'object'\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __init__(self, name: 'str', *, pat_str: 'str', mergeable_ranks: 'dict[bytes, int]', special_tokens: 'dict[str, int]', explicit_n_vocab: 'int | None' = None)\n",
      " |      Creates an Encoding object.\n",
      " |      \n",
      " |      See openai_public.py for examples of how to construct an Encoding object.\n",
      " |      \n",
      " |      Args:\n",
      " |          name: The name of the encoding. It should be clear from the name of the encoding\n",
      " |              what behaviour to expect, in particular, encodings with different special tokens\n",
      " |              should have different names.\n",
      " |          pat_str: A regex pattern string that is used to split the input text.\n",
      " |          mergeable_ranks: A dictionary mapping mergeable token bytes to their ranks. The ranks\n",
      " |              must correspond to merge priority.\n",
      " |          special_tokens: A dictionary mapping special token strings to their token values.\n",
      " |          explicit_n_vocab: The number of tokens in the vocabulary. If provided, it is checked\n",
      " |              that the number of mergeable tokens and special tokens is equal to this number.\n",
      " |  \n",
      " |  __repr__(self) -> 'str'\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, value: 'object') -> 'None'\n",
      " |  \n",
      " |  decode(self, tokens: 'Sequence[int]', errors: 'str' = 'replace') -> 'str'\n",
      " |      Decodes a list of tokens into a string.\n",
      " |      \n",
      " |      WARNING: the default behaviour of this function is lossy, since decoded bytes are not\n",
      " |      guaranteed to be valid UTF-8. You can control this behaviour using the `errors` parameter,\n",
      " |      for instance, setting `errors=strict`.\n",
      " |      \n",
      " |      ```\n",
      " |      >>> enc.decode([31373, 995])\n",
      " |      'hello world'\n",
      " |      ```\n",
      " |  \n",
      " |  decode_batch(self, batch: 'Sequence[Sequence[int]]', *, errors: 'str' = 'replace', num_threads: 'int' = 8) -> 'list[str]'\n",
      " |      Decodes a batch (list of lists of tokens) into a list of strings.\n",
      " |  \n",
      " |  decode_bytes(self, tokens: 'Sequence[int]') -> 'bytes'\n",
      " |      Decodes a list of tokens into bytes.\n",
      " |      \n",
      " |      ```\n",
      " |      >>> enc.decode_bytes([31373, 995])\n",
      " |      b'hello world'\n",
      " |      ```\n",
      " |  \n",
      " |  decode_bytes_batch(self, batch: 'Sequence[Sequence[int]]', *, num_threads: 'int' = 8) -> 'list[bytes]'\n",
      " |      Decodes a batch (list of lists of tokens) into a list of bytes.\n",
      " |  \n",
      " |  decode_single_token_bytes(self, token: 'int') -> 'bytes'\n",
      " |      Decodes a token into bytes.\n",
      " |      \n",
      " |      NOTE: this will decode all special tokens.\n",
      " |      \n",
      " |      Raises `KeyError` if the token is not in the vocabulary.\n",
      " |      \n",
      " |      ```\n",
      " |      >>> enc.decode_single_token_bytes(31373)\n",
      " |      b'hello'\n",
      " |      ```\n",
      " |  \n",
      " |  decode_tokens_bytes(self, tokens: 'Sequence[int]') -> 'list[bytes]'\n",
      " |      Decodes a list of tokens into a list of bytes.\n",
      " |      \n",
      " |      Useful for visualising tokenisation.\n",
      " |      >>> enc.decode_tokens_bytes([31373, 995])\n",
      " |      [b'hello', b' world']\n",
      " |  \n",
      " |  decode_with_offsets(self, tokens: 'Sequence[int]') -> 'tuple[str, list[int]]'\n",
      " |      Decodes a list of tokens into a string and a list of offsets.\n",
      " |      \n",
      " |      Each offset is the index into text corresponding to the start of each token.\n",
      " |      If UTF-8 character boundaries do not line up with token boundaries, the offset is the index\n",
      " |      of the first character that contains bytes from the token.\n",
      " |      \n",
      " |      This will currently raise if given tokens that decode to invalid UTF-8; this behaviour may\n",
      " |      change in the future to be more permissive.\n",
      " |      \n",
      " |      >>> enc.decode_with_offsets([31373, 995])\n",
      " |      ('hello world', [0, 5])\n",
      " |  \n",
      " |  encode(self, text: 'str', *, allowed_special: \"Literal['all'] | AbstractSet[str]\" = set(), disallowed_special: \"Literal['all'] | Collection[str]\" = 'all') -> 'list[int]'\n",
      " |      Encodes a string into tokens.\n",
      " |      \n",
      " |      Special tokens are artificial tokens used to unlock capabilities from a model,\n",
      " |      such as fill-in-the-middle. So we want to be careful about accidentally encoding special\n",
      " |      tokens, since they can be used to trick a model into doing something we don't want it to do.\n",
      " |      \n",
      " |      Hence, by default, encode will raise an error if it encounters text that corresponds\n",
      " |      to a special token. This can be controlled on a per-token level using the `allowed_special`\n",
      " |      and `disallowed_special` parameters. In particular:\n",
      " |      - Setting `disallowed_special` to () will prevent this function from raising errors and\n",
      " |        cause all text corresponding to special tokens to be encoded as natural text.\n",
      " |      - Setting `allowed_special` to \"all\" will cause this function to treat all text\n",
      " |        corresponding to special tokens to be encoded as special tokens.\n",
      " |      \n",
      " |      ```\n",
      " |      >>> enc.encode(\"hello world\")\n",
      " |      [31373, 995]\n",
      " |      >>> enc.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})\n",
      " |      [50256]\n",
      " |      >>> enc.encode(\"<|endoftext|>\", allowed_special=\"all\")\n",
      " |      [50256]\n",
      " |      >>> enc.encode(\"<|endoftext|>\")\n",
      " |      # Raises ValueError\n",
      " |      >>> enc.encode(\"<|endoftext|>\", disallowed_special=())\n",
      " |      [27, 91, 437, 1659, 5239, 91, 29]\n",
      " |      ```\n",
      " |  \n",
      " |  encode_batch(self, text: 'list[str]', *, num_threads: 'int' = 8, allowed_special: \"Literal['all'] | AbstractSet[str]\" = set(), disallowed_special: \"Literal['all'] | Collection[str]\" = 'all') -> 'list[list[int]]'\n",
      " |      Encodes a list of strings into tokens, in parallel.\n",
      " |      \n",
      " |      See `encode` for more details on `allowed_special` and `disallowed_special`.\n",
      " |      \n",
      " |      ```\n",
      " |      >>> enc.encode_batch([\"hello world\", \"goodbye world\"])\n",
      " |      [[31373, 995], [11274, 16390, 995]]\n",
      " |      ```\n",
      " |  \n",
      " |  encode_ordinary(self, text: 'str') -> 'list[int]'\n",
      " |      Encodes a string into tokens, ignoring special tokens.\n",
      " |      \n",
      " |      This is equivalent to `encode(text, disallowed_special=())` (but slightly faster).\n",
      " |      \n",
      " |      ```\n",
      " |      >>> enc.encode_ordinary(\"hello world\")\n",
      " |      [31373, 995]\n",
      " |  \n",
      " |  encode_ordinary_batch(self, text: 'list[str]', *, num_threads: 'int' = 8) -> 'list[list[int]]'\n",
      " |      Encodes a list of strings into tokens, in parallel, ignoring special tokens.\n",
      " |      \n",
      " |      This is equivalent to `encode_batch(text, disallowed_special=())` (but slightly faster).\n",
      " |      \n",
      " |      ```\n",
      " |      >>> enc.encode_ordinary_batch([\"hello world\", \"goodbye world\"])\n",
      " |      [[31373, 995], [11274, 16390, 995]]\n",
      " |      ```\n",
      " |  \n",
      " |  encode_single_token(self, text_or_bytes: 'str | bytes') -> 'int'\n",
      " |      Encodes text corresponding to a single token to its token value.\n",
      " |      \n",
      " |      NOTE: this will encode all special tokens.\n",
      " |      \n",
      " |      Raises `KeyError` if the token is not in the vocabulary.\n",
      " |      \n",
      " |      ```\n",
      " |      >>> enc.encode_single_token(\"hello\")\n",
      " |      31373\n",
      " |      ```\n",
      " |  \n",
      " |  encode_to_numpy(self, text: 'str', *, allowed_special: \"Literal['all'] | AbstractSet[str]\" = set(), disallowed_special: \"Literal['all'] | Collection[str]\" = 'all') -> 'npt.NDArray[np.uint32]'\n",
      " |      Encodes a string into tokens, returning a numpy array.\n",
      " |      \n",
      " |      Avoids the overhead of copying the token buffer into a Python list.\n",
      " |  \n",
      " |  encode_with_unstable(self, text: 'str', *, allowed_special: \"Literal['all'] | AbstractSet[str]\" = set(), disallowed_special: \"Literal['all'] | Collection[str]\" = 'all') -> 'tuple[list[int], list[list[int]]]'\n",
      " |      Encodes a string into stable tokens and possible completion sequences.\n",
      " |      \n",
      " |      Note that the stable tokens will only represent a substring of `text`.\n",
      " |      \n",
      " |      See `encode` for more details on `allowed_special` and `disallowed_special`.\n",
      " |      \n",
      " |      This API should itself be considered unstable.\n",
      " |      \n",
      " |      ```\n",
      " |      >>> enc.encode_with_unstable(\"hello fanta\")\n",
      " |      ([31373], [(277, 4910), (5113, 265), ..., (8842,)])\n",
      " |      \n",
      " |      >>> text = \"...\"\n",
      " |      >>> stable_tokens, completions = enc.encode_with_unstable(text)\n",
      " |      >>> assert text.encode().startswith(enc.decode_bytes(stable_tokens))\n",
      " |      >>> assert all(enc.decode_bytes(stable_tokens + seq).startswith(text.encode()) for seq in completions)\n",
      " |      ```\n",
      " |  \n",
      " |  is_special_token(self, token: 'int') -> 'bool'\n",
      " |  \n",
      " |  special_tokens_set = <functools.cached_property object>\n",
      " |  token_byte_values(self) -> 'list[bytes]'\n",
      " |      Returns the list of all token byte values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  eot_token\n",
      " |  \n",
      " |  n_vocab\n",
      " |      For backwards compatibility. Prefer to use `enc.max_token_value + 1`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f8ec5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100277\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.n_vocab\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "718217c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9906, 11, 656, 499, 1093, 15600, 30, 220, 100257, 763, 279, 7160, 32735, 7317, 2492, 315, 1063, 16476, 17826, 13]\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\"\n",
    "\n",
    "intergers = tokenizer.encode(text,allowed_special = {\"<|endoftext|>\"})\n",
    "# 特殊 token 是一个特殊 ID，不能通过普通 BPE 组合得到。\n",
    "# 例如，在 cl100k_base 或 gpt2 编码中：\n",
    "# <|endoftext|> 的 ID 是 100257\n",
    "# 它必须在 vocab 的 特殊 token map 中才能被识别\n",
    "# 如果没有显式允许，它会被“拆碎”成若干普通 token（普通词+符号）\n",
    "print(intergers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3cebd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9906, 11, 656, 499, 1093, 15600, 30, 83739, 8862, 728, 428, 91, 29, 763, 279, 7160, 32735, 7317, 2492, 315, 1063, 16476, 17826, 13]\n"
     ]
    }
   ],
   "source": [
    "intergers = tokenizer.encode(text,disallowed_special=())\n",
    "#你禁用了所有特殊 token 的限制 → 全部特殊 token 都被当作普通文本处理\n",
    "#< | end of text | >   ← BPE 分成很多小 token\n",
    "# ... [30, 83739, 8862, 728, 428, 91, 29, ...]\n",
    "print(intergers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "237fba6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings = tokenizer.decode(intergers)\n",
    "strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4d46c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32, 29700, 404, 86, 602, 261]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"Akwirw ier\"\n",
    "idx = tokenizer.encode(word)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "702c398c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Akwirw ier'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = tokenizer.decode(idx)\n",
    "string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df4c085",
   "metadata": {},
   "source": [
    "##  2.6 使用滑动窗口进行数据采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a1e3580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4943\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c2fc645a",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_example = enc_text[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c3414e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([323, 9749, 5678, 304], [9749, 5678, 304, 264])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_size = 4\n",
    "x = enc_example[0:context_size]\n",
    "y = enc_example[1:context_size+1]\n",
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "71a5d2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[323] ----> 9749\n",
      "[323, 9749] ----> 5678\n",
      "[323, 9749, 5678] ----> 304\n",
      "[323, 9749, 5678, 304] ----> 264\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,context_size+1):\n",
    "    context = enc_example[:i]\n",
    "    desired = enc_example[i]\n",
    "    print(context,\"---->\",desired)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75aa5ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,context_size+1):\n",
    "    context = enc_example[:i]\n",
    "    desired = enc_example[i]\n",
    "    print(tokenizer.decode(context),\"---->\",tokenizer.decode([desired]))\n",
    "    # 输入需要是序列\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "92a80a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self,txt,tokenizer,max_length,stride):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "        #A 将整个文本进行分词\n",
    "\n",
    "        for i in range(0,len(token_ids)-max_length,stride):\n",
    "            #B 使用滑动窗口将书籍分块为最大长度的重叠序列。\n",
    "            input_chunk = token_ids[i:i+max_length]\n",
    "            # 这样能够确保最大的长度不超过tokenids的长度\n",
    "            target_chunk = token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    #C 返回数据集的总行数\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.input_ids[idx],self.target_ids[idx]\n",
    "        #D 从数据集中返回指定行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d8c41d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt,batch_size=4,max_length=256,stride=128,shuffle=True,drop_last=True,num_workers=0):\n",
    "    tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    dataset = GPTDatasetV1(txt,tokenizer,max_length,stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        #D 用于预处理的CPU进程数量\n",
    "        drop_last = drop_last,\n",
    "        #C drop_last=True会在最后一批次小于指定的batch_size时丢弃该批次，以防止训练期间的损失峰值\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d472a2d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[  40,  473, 1846, 2744]]), tensor([[ 473, 1846, 2744, 3463]])]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\",\"r\",encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "data_loader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(data_loader)\n",
    "next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2062fd51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 473, 1846, 2744, 3463]]), tensor([[1846, 2744, 3463, 7762]])]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "92437e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[ 4856, 15262,   279,  4623],\n",
      "        [  596, 10788,   574,   779],\n",
      "        [ 1306,  1461,    11, 17948],\n",
      "        [   13,   662,   662,  6905],\n",
      "        [ 8835,    11,   439,   568],\n",
      "        [ 4265,  4856,  1093,   311],\n",
      "        [  832,  1317,   291,   311],\n",
      "        [  313,  1494,    11,  8009]])\n",
      "\n",
      "Targets:\n",
      " tensor([[15262,   279,  4623,   313],\n",
      "        [10788,   574,   779,  4173],\n",
      "        [ 1461,    11, 17948,   555],\n",
      "        [  662,   662,  6905, 46639],\n",
      "        [   11,   439,   568, 16392],\n",
      "        [ 4856,  1093,   311,  3371],\n",
      "        [ 1317,   291,   311, 16106],\n",
      "        [ 1494,    11,  8009,  4610]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc4bbb4",
   "metadata": {},
   "source": [
    "## 2.7 构建词嵌入层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dcd574e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "895a4d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4cae1952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e609d8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6f6a149e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5322db1",
   "metadata": {},
   "source": [
    "## 2.8 位置编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5e35404d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab_size = 100277\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0eeb5159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   473,  1846,  2744],\n",
      "        [ 3463,  7762,   480,   285],\n",
      "        [22464,  4856,   264, 12136],\n",
      "        [35201,   313,  4636,   264],\n",
      "        [ 1695, 12637,  3403,   313],\n",
      "        [  708,   433,   574,   912],\n",
      "        [ 2294, 13051,   311,   757],\n",
      "        [  311,  6865,   430,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "  \traw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)\n",
    "#size = batch_size*max_length 8*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dea5c24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "04907143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.9533, -1.4118, -0.1707,  ..., -0.0447, -0.2458, -0.2742],\n",
      "        [ 0.7352, -0.2087,  0.0884,  ...,  0.4836, -0.4149,  1.1965],\n",
      "        [ 0.6648, -0.6451, -1.1558,  ...,  0.1560,  1.0606, -1.2221],\n",
      "        [ 0.5973, -0.2016, -0.2678,  ..., -0.1727, -0.6388, -0.2301]],\n",
      "       requires_grad=True)\n",
      "torch.Size([4, 256])\n",
      "tensor([[-1.9533, -1.4118, -0.1707,  ..., -0.0447, -0.2458, -0.2742],\n",
      "        [ 0.7352, -0.2087,  0.0884,  ...,  0.4836, -0.4149,  1.1965],\n",
      "        [ 0.6648, -0.6451, -1.1558,  ...,  0.1560,  1.0606, -1.2221],\n",
      "        [ 0.5973, -0.2016, -0.2678,  ..., -0.1727, -0.6388, -0.2301]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "print(pos_embedding_layer.weight)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)\n",
    "print(pos_embeddings)\n",
    "#.weight = 所有位置的embedding参数\n",
    "#pos_embedding_layer(索引) = 按需查表，输出“实际要用的那些位置向量”\n",
    "# 对于 GPT 模型所使用的绝对嵌入方法，我们只需创建另一个嵌入层，\n",
    "# 其维度与 token_embedding_layer 的维度相同："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "53cc09e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 256])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings = pos_embeddings+token_embeddings\n",
    "input_embeddings.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
